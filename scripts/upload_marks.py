#!/usr/bin/env python
# -*- coding: utf- -*-

from marking import canvas_api, mongodb_store, marking_actions, marks, file_actions, git_actions, java_actions
import requests
from marking.mongodb_store import content_decode


if __name__ == "__main__":
    store = mongodb_store.SubmissionStore()
    capi = canvas_api.CanvasAPI(store.get_key())

    parser = argparse.ArgumentParser(description='Fetch, store then export submitted attachments for an assignment')
    parser.add_argument('course_id', metavar='CID', type=str, nargs=1,
                   help='The Canvas course ID. If unknown you can use list_courses.py')
    parser.add_argument('assignment_id', metavar='AID', type=str, nargs=1,   
                   help='The Canvas assignment ID. If unknown you can use list_assignments.py')

    args = parser.parse_args()     

    course_id = args.course_id[0]
    assignment_id = args.assignment_id[0]


    # get all assignments
    submissions = store.get_assignment_submissions(course_id, assignment_id)        

    print('%s submissions to upload' % len(submissions))

    count = 1
    for submission in submissions:
        print ('%s %s/%s' % (submission['user_id'], count, len(submissions)))
        if 'marks' in submission:

            preliminary = False

            if preliminary:
                grade_comment = '----------------------\nTHESE ARE PRELIMINARY MARKS.\nThe automated script will be rerun after the submission deadline with a small number of extra tests. These comments are just for feedback. You can still alter your submission up until the deadline. The marks indicated will change as futher tests are introduced.\n'

                grade_comment += '\n *** Grade is set to 0 as marks are not final. ***\n'           

            else:
                grade_comment = '----------------------\nThese are your marks as generated by the automated mark script. If you wish to discuss the marks, please DO NOT comment here as responses are hard to track. Instead please email Nick.\n'

            grade, comment = marks.aggregate_marks(submission['marks'])
    
            grade_comment += '\n\n# Setup Output (cloning, compilation)\n'
            grade_comment += comment + '\n'

            grade_comment += '\n\n# Test Output\n\n'

            if 'auto_comments' in submission:
                grade_comment += submission['auto_comments']
                grade += submission['auto_mark']
            else:
                grade_comment += 'Automated tests did not generate a mark. This indicates either a cloning, compilation or serious runtime error.'

            grade_comment += '\n----------------------\n'

            if preliminary:
                print grade_comment
                print grade
                grade = 0
                



            capi.grade_assignment_submission(course_id, assignment_id, submission['user_id'], grade, grade_comment)
            count += 1
            # break
